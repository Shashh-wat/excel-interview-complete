# interview_orchestrator.py - COMPLETE WORKING Interview Orchestrator with Voice Integration
"""
Complete interview orchestrator with proper error handling, session management,
voice integration, and all existing functionality preserved and enhanced.
"""

import asyncio
import logging
import uuid
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from pathlib import Path

# Import your existing components
try:
    from evaluation_engine import ClaudeEvaluationEngine
    EVALUATION_ENGINE_AVAILABLE = True
except ImportError:
    EVALUATION_ENGINE_AVAILABLE = False
    logging.warning("evaluation_engine not available")

try:
    from models import (
        InterviewSession, Question, InterviewStatus,
        SkillCategory, DifficultyLevel, QuestionType,
        create_interview_session, create_question
    )
    MODELS_AVAILABLE = True
except ImportError:
    MODELS_AVAILABLE = False
    logging.warning("models not available")

# Configure logging
logger = logging.getLogger(__name__)

# =============================================================================
# FALLBACK MODELS (if models.py not available)
# =============================================================================

if not MODELS_AVAILABLE:
    from enum import Enum
    from dataclasses import dataclass
    from datetime import datetime
    
    class InterviewStatus(Enum):
        NOT_STARTED = "not_started"
        IN_PROGRESS = "in_progress" 
        COMPLETED = "completed"
        ERROR = "error"
    
    class SkillCategory(Enum):
        FORMULAS = "formulas"
        DATA_MANIPULATION = "data_manipulation"
        PIVOT_TABLES = "pivot_tables"
        ERROR_HANDLING = "error_handling"
    
    class DifficultyLevel(Enum):
        BEGINNER = "beginner"
        INTERMEDIATE = "intermediate"
        ADVANCED = "advanced"
    
    class QuestionType(Enum):
        FREE_TEXT = "free_text"
        FILE_UPLOAD = "file_upload"
    
    @dataclass
    class Question:
        id: str
        text: str
        type: str
        skill_category: str
        difficulty: str
        expected_keywords: List[str] = None
        
        def to_dict(self):
            return {
                "id": self.id,
                "text": self.text,
                "type": self.type,
                "skill_category": self.skill_category,
                "difficulty": self.difficulty,
                "expected_keywords": self.expected_keywords or []
            }

# =============================================================================
# ENHANCED QUESTION BANK (Built-in fallback with more questions)
# =============================================================================

class SimpleQuestionBank:
    """Enhanced question bank with Excel questions and voice support"""
    
    def __init__(self):
        self.questions = [
            {
                "id": "excel_q1",
                "text": "Explain the difference between VLOOKUP and INDEX-MATCH functions in Excel. When would you use each one?",
                "type": "free_text",
                "skill_category": "formulas",
                "difficulty": "intermediate",
                "expected_keywords": ["VLOOKUP", "INDEX", "MATCH", "lookup", "table", "flexible", "array"]
            },
            {
                "id": "excel_q2", 
                "text": "How do you create a pivot table in Excel? Describe the key steps and what type of data analysis it's useful for.",
                "type": "free_text",
                "skill_category": "pivot_tables",
                "difficulty": "intermediate",
                "expected_keywords": ["pivot table", "summarize", "data", "rows", "columns", "values", "filter"]
            },
            {
                "id": "excel_q3",
                "text": "What are some common Excel errors like hash N A, hash REF, hash VALUE and how would you troubleshoot them?",
                "type": "free_text", 
                "skill_category": "error_handling",
                "difficulty": "intermediate",
                "expected_keywords": ["#N/A", "#REF!", "#VALUE!", "error", "troubleshoot", "IFERROR", "debug"]
            },
            {
                "id": "excel_q4",
                "text": "Describe how to use conditional formatting in Excel to highlight data based on specific criteria.",
                "type": "free_text",
                "skill_category": "data_manipulation",
                "difficulty": "beginner",
                "expected_keywords": ["conditional formatting", "highlight", "criteria", "rules", "format", "cells"]
            },
            {
                "id": "excel_q5",
                "text": "Explain how you would use SUMIF and COUNTIF functions. Provide examples of when they're useful.",
                "type": "free_text",
                "skill_category": "formulas", 
                "difficulty": "intermediate",
                "expected_keywords": ["SUMIF", "COUNTIF", "criteria", "range", "condition", "aggregate"]
            },
            {
                "id": "excel_q6",
                "text": "How do you protect cells and worksheets in Excel? What are the different protection options available?",
                "type": "free_text",
                "skill_category": "data_manipulation",
                "difficulty": "advanced",
                "expected_keywords": ["protect", "lock", "password", "worksheet", "cells", "security", "permissions"]
            },
            {
                "id": "excel_q7",
                "text": "What is the difference between absolute and relative cell references? Give examples of when to use each.",
                "type": "free_text",
                "skill_category": "formulas",
                "difficulty": "beginner", 
                "expected_keywords": ["absolute", "relative", "reference", "$", "copy", "formula", "F4"]
            },
            {
                "id": "excel_q8",
                "text": "Describe how to create charts in Excel. What are the different chart types and when would you use each?",
                "type": "free_text",
                "skill_category": "data_manipulation",
                "difficulty": "intermediate",
                "expected_keywords": ["chart", "graph", "bar", "line", "pie", "visualization", "data"]
            },
            {
                "id": "excel_q9",
                "text": "Explain how to use Excel's Goal Seek and Solver features. What business problems do they help solve?",
                "type": "free_text",
                "skill_category": "data_manipulation",
                "difficulty": "advanced",
                "expected_keywords": ["Goal Seek", "Solver", "optimization", "what-if", "analysis", "constraints"]
            },
            {
                "id": "excel_q10",
                "text": "How do you work with multiple worksheets and workbooks in Excel? Describe linking and consolidation techniques.",
                "type": "free_text",
                "skill_category": "data_manipulation",
                "difficulty": "advanced",
                "expected_keywords": ["worksheet", "workbook", "link", "consolidate", "reference", "3D"]
            }
        ]
        self.current_index = 0
    
    async def get_question(self, strategy="sequential", **kwargs):
        """Get next question with enhanced strategy support"""
        if self.current_index >= len(self.questions):
            return None
        
        question_data = self.questions[self.current_index]
        self.current_index += 1
        
        if MODELS_AVAILABLE:
            return Question(**question_data)
        else:
            return Question(
                id=question_data["id"],
                text=question_data["text"],
                type=question_data["type"],
                skill_category=question_data["skill_category"],
                difficulty=question_data["difficulty"],
                expected_keywords=question_data["expected_keywords"]
            )
    
    async def get_question_bank_status(self):
        """Get detailed status"""
        return {
            "initialized": True,
            "total_questions": len(self.questions),
            "remaining_questions": len(self.questions) - self.current_index,
            "current_index": self.current_index,
            "categories": list(set(q["skill_category"] for q in self.questions)),
            "difficulty_levels": list(set(q["difficulty"] for q in self.questions))
        }
    
    def reset(self):
        """Reset question bank to start"""
        self.current_index = 0

# =============================================================================
# CORE INTERVIEW ORCHESTRATOR (Enhanced with all existing functionality)
# =============================================================================

class InterviewOrchestrator:
    """Main interview orchestrator with comprehensive session management"""
    
    def __init__(self, evaluation_engine=None, question_bank=None):
        self.evaluation_engine = evaluation_engine
        self.question_bank = question_bank or SimpleQuestionBank()
        self.sessions = {}
        self.logger = logging.getLogger(__name__)
        
        # Initialize session storage
        self.active_sessions = self.sessions  # Alias for compatibility
        
        # Performance tracking
        self.stats = {
            "total_interviews_started": 0,
            "total_interviews_completed": 0,
            "total_responses_processed": 0,
            "average_completion_time": 0.0,
            "average_score": 0.0
        }
        
        self.logger.info("âœ… InterviewOrchestrator initialized")
    
    async def start_interview(self, candidate_name: str = None, skill_level: str = None) -> Dict[str, Any]:
        """Start a new interview session with enhanced tracking"""
        
        try:
            # Generate session ID
            session_id = f"interview_{uuid.uuid4().hex[:12]}"
            
            # Create comprehensive session data
            session_data = {
                "session_id": session_id,
                "candidate_name": candidate_name or "Anonymous",
                "status": "in_progress",
                "start_time": datetime.utcnow(),
                "questions_asked": [],
                "evaluations": [],
                "current_question_index": 0,
                "total_questions_planned": 10,
                "skills_covered": {},
                "conversation_history": [],
                "skill_level": skill_level or "adaptive",
                "metadata": {
                    "user_agent": "interview_system",
                    "session_type": "standard",
                    "created_at": datetime.utcnow().isoformat()
                }
            }
            
            # Store session
            self.sessions[session_id] = session_data
            
            # Update stats
            self.stats["total_interviews_started"] += 1
            
            # Get first question
            first_question = await self.question_bank.get_question(strategy="sequential")
            
            if first_question:
                first_question_data = first_question.to_dict() if hasattr(first_question, 'to_dict') else {
                    "id": first_question.id,
                    "text": first_question.text,
                    "type": first_question.type,
                    "skill_category": first_question.skill_category,
                    "difficulty": first_question.difficulty,
                    "expected_keywords": getattr(first_question, 'expected_keywords', [])
                }
                
                session_data["current_question"] = first_question_data
                
                self.logger.info(f"âœ… Interview started: {session_id} for {candidate_name}")
                
                return {
                    "success": True,
                    "session_id": session_id,
                    "status": "started",
                    "first_question": first_question_data,
                    "candidate_name": candidate_name,
                    "estimated_duration": "15-20 minutes",
                    "total_questions": session_data["total_questions_planned"],
                    "session_metadata": {
                        "skill_level": skill_level,
                        "question_bank_size": len(self.question_bank.questions),
                        "evaluation_engine_available": self.evaluation_engine is not None
                    }
                }
            else:
                # No questions available
                return {
                    "success": False,
                    "error": "No questions available in question bank",
                    "status": "error"
                }
        
        except Exception as e:
            self.logger.error(f"âŒ Failed to start interview: {e}")
            return {
                "success": False,
                "error": str(e),
                "status": "error"
            }
    
    async def submit_response(
        self, 
        session_id: str, 
        response_text: str, 
        time_taken: int = 0,
        file_path: str = None
    ) -> Dict[str, Any]:
        """Submit and evaluate a response with enhanced tracking"""
        
        try:
            # Check if session exists
            if session_id not in self.sessions:
                return {
                    "success": False,
                    "error": "Session not found",
                    "status": "error"
                }
            
            session_data = self.sessions[session_id]
            
            # Check if interview is already completed
            if session_data.get("status") == "completed":
                return {
                    "success": False,
                    "error": "Interview already completed",
                    "status": "completed"
                }
            
            # Get current question
            current_question = session_data.get("current_question")
            if not current_question:
                return {
                    "success": False,
                    "error": "No current question found",
                    "status": "error"
                }
            
            # Evaluate response
            evaluation = await self._evaluate_response(current_question, response_text, file_path)
            
            # Record response and evaluation
            session_data["questions_asked"].append(current_question["id"])
            session_data["evaluations"].append(evaluation)
            session_data["current_question_index"] += 1
            
            # Update stats
            self.stats["total_responses_processed"] += 1
            
            # Update skills covered
            skill = current_question.get("skill_category", "general")
            session_data["skills_covered"][skill] = session_data["skills_covered"].get(skill, 0) + 1
            
            # Add to conversation history
            session_data["conversation_history"].extend([
                {
                    "type": "question",
                    "content": current_question["text"],
                    "question_id": current_question["id"],
                    "timestamp": datetime.utcnow().isoformat()
                },
                {
                    "type": "response", 
                    "content": response_text,
                    "evaluation_score": evaluation.get("score", 0),
                    "time_taken": time_taken,
                    "file_provided": file_path is not None,
                    "timestamp": datetime.utcnow().isoformat()
                }
            ])
            
            # Check if interview should continue
            questions_completed = len(session_data["questions_asked"])
            max_questions = session_data["total_questions_planned"]
            
            # Get next question
            next_question = None
            if questions_completed < max_questions:
                next_question = await self.question_bank.get_question(strategy="sequential")
                if next_question:
                    next_question_data = next_question.to_dict() if hasattr(next_question, 'to_dict') else {
                        "id": next_question.id,
                        "text": next_question.text,
                        "type": next_question.type,
                        "skill_category": next_question.skill_category,
                        "difficulty": next_question.difficulty,
                        "expected_keywords": getattr(next_question, 'expected_keywords', [])
                    }
                    session_data["current_question"] = next_question_data
                else:
                    # No more questions available
                    next_question = None
            
            # Determine if interview is complete
            interview_complete = (
                questions_completed >= max_questions or 
                next_question is None or
                questions_completed >= 15  # Hard limit
            )
            
            if interview_complete:
                session_data["status"] = "completed"
                session_data["end_time"] = datetime.utcnow()
                session_data["final_score"] = self._calculate_final_score(session_data["evaluations"])
                
                # Update completion stats
                self.stats["total_interviews_completed"] += 1
                self._update_average_stats(session_data)
            
            # Calculate progress
            progress_percentage = (questions_completed / max_questions) * 100
            
            # Prepare comprehensive response
            response_data = {
                "success": True,
                "evaluation": evaluation,
                "next_question": next_question_data if next_question and not interview_complete else None,
                "status": "completed" if interview_complete else "in_progress", 
                "progress": {
                    "questions_completed": questions_completed,
                    "total_questions": max_questions,
                    "progress_percentage": min(progress_percentage, 100),
                    "skills_covered": len(session_data["skills_covered"]),
                    "current_average_score": self._calculate_final_score(session_data["evaluations"])
                },
                "session_stats": {
                    "duration_minutes": self._calculate_duration(session_data),
                    "responses_count": len(session_data["evaluations"]),
                    "skills_distribution": session_data["skills_covered"]
                }
            }
            
            if interview_complete:
                response_data["completion_message"] = f"ðŸŽ‰ Interview completed! You answered {questions_completed} questions."
                response_data["final_results"] = {
                    "final_score": session_data["final_score"],
                    "questions_completed": questions_completed,
                    "skills_covered": list(session_data["skills_covered"].keys()),
                    "duration_minutes": self._calculate_duration(session_data),
                    "performance_level": self._get_performance_level(session_data["final_score"])
                }
            
            return response_data
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to submit response: {e}")
            return {
                "success": False,
                "error": str(e),
                "status": "error"
            }
    
    async def _evaluate_response(self, question: Dict, response_text: str, file_path: str = None) -> Dict[str, Any]:
        """Evaluate a response using the evaluation engine with fallback"""
        
        try:
            if self.evaluation_engine and EVALUATION_ENGINE_AVAILABLE:
                # Use the real evaluation engine
                evaluation = await self.evaluation_engine.evaluate_response(
                    question=question,
                    text_response=response_text,
                    file_path=file_path
                )
                return evaluation
            else:
                # Enhanced fallback evaluation
                return self._fallback_evaluation(question, response_text)
                
        except Exception as e:
            self.logger.warning(f"Evaluation failed, using fallback: {e}")
            return self._fallback_evaluation(question, response_text)
    
    def _fallback_evaluation(self, question: Dict, response_text: str) -> Dict[str, Any]:
        """Enhanced fallback evaluation when Claude is not available"""
        
        if not response_text or not response_text.strip():
            return {
                "score": 0.0,
                "confidence": 1.0,
                "reasoning": "No response provided",
                "strengths": [],
                "areas_for_improvement": ["Please provide a response"],
                "keywords_found": [],
                "mistakes_detected": []
            }
        
        # Enhanced keyword-based scoring
        expected_keywords = question.get("expected_keywords", [])
        keywords_found = []
        
        if expected_keywords:
            response_lower = response_text.lower()
            keywords_found = [kw for kw in expected_keywords if kw.lower() in response_lower]
        
        # Enhanced scoring logic
        base_score = 1.5  # Base for providing any response
        keyword_bonus = min(2.5, len(keywords_found) * 0.4)  # Up to 2.5 points for keywords
        length_bonus = min(1.0, len(response_text.split()) / 25)  # Up to 1 point for length
        
        # Difficulty adjustment
        difficulty = question.get("difficulty", "intermediate")
        if difficulty == "beginner" and len(keywords_found) >= 2:
            base_score += 0.5
        elif difficulty == "advanced" and len(keywords_found) >= 3:
            base_score += 0.5
        
        final_score = min(5.0, base_score + keyword_bonus + length_bonus)
        
        # Generate contextual feedback
        strengths = []
        improvements = []
        
        if keywords_found:
            strengths.extend([f"Mentioned {kw}" for kw in keywords_found[:3]])
        else:
            improvements.append("Include more Excel-specific terminology")
        
        if len(response_text.split()) >= 50:
            strengths.append("Provided detailed explanation")
        else:
            improvements.append("Could provide more detailed explanation")
        
        if not strengths:
            strengths = ["Response provided"]
        
        if not improvements:
            improvements = ["Great response!"]
        
        return {
            "score": round(final_score, 1),
            "confidence": 0.7,
            "reasoning": f"Found {len(keywords_found)} relevant keywords in {len(response_text.split())} word response. {difficulty.title()} level question.",
            "strengths": strengths,
            "areas_for_improvement": improvements,
            "keywords_found": keywords_found,
            "mistakes_detected": [],
            "evaluation_method": "fallback_enhanced"
        }
    
    def _calculate_final_score(self, evaluations: List[Dict]) -> float:
        """Calculate final score from all evaluations"""
        if not evaluations:
            return 0.0
        
        total_score = sum(eval_data.get("score", 0) for eval_data in evaluations)
        return round(total_score / len(evaluations), 2)
    
    def _calculate_duration(self, session_data: Dict) -> int:
        """Calculate interview duration in minutes"""
        start_time = session_data.get("start_time")
        end_time = session_data.get("end_time", datetime.utcnow())
        
        if start_time and end_time:
            duration = end_time - start_time
            return int(duration.total_seconds() / 60)
        return 0
    
    def _get_performance_level(self, score: float) -> str:
        """Get performance level based on score"""
        if score >= 4.5:
            return "Expert"
        elif score >= 3.5:
            return "Advanced"
        elif score >= 2.5:
            return "Intermediate"
        elif score >= 1.5:
            return "Beginner"
        else:
            return "Needs Practice"
    
    def _update_average_stats(self, session_data: Dict):
        """Update running averages"""
        try:
            final_score = session_data.get("final_score", 0)
            duration = self._calculate_duration(session_data)
            
            # Update average score
            total_completed = self.stats["total_interviews_completed"]
            if total_completed == 1:
                self.stats["average_score"] = final_score
                self.stats["average_completion_time"] = duration
            else:
                alpha = 1.0 / total_completed
                self.stats["average_score"] = (1 - alpha) * self.stats["average_score"] + alpha * final_score
                self.stats["average_completion_time"] = (1 - alpha) * self.stats["average_completion_time"] + alpha * duration
        except Exception as e:
            self.logger.warning(f"Stats update failed: {e}")
    
    async def get_session_status(self, session_id: str) -> Dict[str, Any]:
        """Get comprehensive session status"""
        
        try:
            if session_id not in self.sessions:
                return {
                    "success": False,
                    "error": "Session not found",
                    "status": "error"
                }
            
            session_data = self.sessions[session_id]
            
            return {
                "success": True,
                "session_id": session_id,
                "status": session_data.get("status", "unknown"),
                "candidate_name": session_data.get("candidate_name"),
                "questions_completed": len(session_data.get("questions_asked", [])),
                "current_score": self._calculate_final_score(session_data.get("evaluations", [])),
                "skills_covered": len(session_data.get("skills_covered", {})),
                "duration_minutes": self._calculate_duration(session_data),
                "progress_percentage": (len(session_data.get("questions_asked", [])) / session_data.get("total_questions_planned", 10)) * 100,
                "skill_distribution": session_data.get("skills_covered", {}),
                "start_time": session_data.get("start_time", datetime.utcnow()).isoformat(),
                "conversation_length": len(session_data.get("conversation_history", []))
            }
            
        except Exception as e:
            self.logger.error(f"Failed to get session status: {e}")
            return {
                "success": False,
                "error": str(e),
                "status": "error"
            }
    
    async def get_all_sessions(self) -> Dict[str, Any]:
        """Get all session summaries"""
        
        try:
            sessions_summary = []
            
            for session_id, session_data in self.sessions.items():
                sessions_summary.append({
                    "session_id": session_id,
                    "candidate_name": session_data.get("candidate_name"),
                    "status": session_data.get("status"),
                    "questions_completed": len(session_data.get("questions_asked", [])),
                    "current_score": self._calculate_final_score(session_data.get("evaluations", [])),
                    "duration_minutes": self._calculate_duration(session_data),
                    "start_time": session_data.get("start_time", datetime.utcnow()).isoformat()
                })
            
            return {
                "success": True,
                "total_sessions": len(self.sessions),
                "sessions": sessions_summary,
                "system_stats": self.get_system_stats()
            }
            
        except Exception as e:
            self.logger.error(f"Failed to get all sessions: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Get comprehensive system statistics"""
        
        return {
            "interviews_started": self.stats["total_interviews_started"],
            "interviews_completed": self.stats["total_interviews_completed"],
            "responses_processed": self.stats["total_responses_processed"],
            "completion_rate": (self.stats["total_interviews_completed"] / max(self.stats["total_interviews_started"], 1)) * 100,
            "average_score": round(self.stats["average_score"], 2),
            "average_completion_time_minutes": round(self.stats["average_completion_time"], 1),
            "active_sessions": len([s for s in self.sessions.values() if s.get("status") == "in_progress"]),
            "evaluation_engine_available": self.evaluation_engine is not None,
            "question_bank_status": {
                "total_questions": len(self.question_bank.questions),
                "current_index": self.question_bank.current_index
            }
        }
    
    async def generate_final_report(self, session_id: str) -> str:
        """Generate comprehensive final interview report"""
        
        try:
            if session_id not in self.sessions:
                return f"Report generation failed: Session {session_id} not found"
            
            session_data = self.sessions[session_id]
            evaluations = session_data.get("evaluations", [])
            
            if not evaluations:
                return f"Report generation failed: No evaluations found for session {session_id}"
            
            # Calculate comprehensive statistics
            final_score = self._calculate_final_score(evaluations)
            questions_completed = len(session_data.get("questions_asked", []))
            skills_covered = session_data.get("skills_covered", {})
            duration = self._calculate_duration(session_data)
            performance_level = self._get_performance_level(final_score)
            
            # Generate detailed report
            report = f"""
# Excel Skills Assessment Report

**Candidate:** {session_data.get('candidate_name', 'Anonymous')}
**Date:** {session_data.get('start_time', datetime.utcnow()).strftime('%Y-%m-%d %H:%M')}
**Session ID:** {session_id}
**Performance Level:** {performance_level}

## Executive Summary
- **Questions Completed:** {questions_completed}
- **Final Score:** {final_score}/5.0 ({(final_score/5)*100:.1f}%)
- **Skills Assessed:** {len(skills_covered)} categories
- **Duration:** {duration} minutes
- **Status:** {session_data.get('status', 'Unknown')}

## Detailed Performance Analysis

### Score Breakdown
"""
            
            # Add score distribution
            scores = [eval_data.get("score", 0) for eval_data in evaluations]
            if scores:
                max_score = max(scores)
                min_score = min(scores)
                avg_score = sum(scores) / len(scores)
                
                report += f"""
- **Highest Score:** {max_score:.1f}/5.0
- **Lowest Score:** {min_score:.1f}/5.0
- **Average Score:** {avg_score:.1f}/5.0
- **Consistency:** {'High' if (max_score - min_score) <= 1.5 else 'Medium' if (max_score - min_score) <= 2.5 else 'Variable'}
"""
            
            # Skills assessment
            report += f"""
### Skills Assessment
"""
            
            for skill, count in skills_covered.items():
                skill_name = skill.replace('_', ' ').title()
                # Calculate average score for this skill
                skill_scores = []
                for i, eval_data in enumerate(evaluations):
                    if i < len(session_data.get("questions_asked", [])):
                        question_id = session_data["questions_asked"][i]
                        # Find question by ID
                        for q in self.question_bank.questions:
                            if q["id"] == question_id and q["skill_category"] == skill:
                                skill_scores.append(eval_data.get("score", 0))
                
                avg_skill_score = sum(skill_scores) / len(skill_scores) if skill_scores else 0
                report += f"- **{skill_name}:** {count} question(s), Average: {avg_skill_score:.1f}/5.0\n"
            
            # Performance feedback
            if final_score >= 4.0:
                performance_feedback = "Outstanding Excel expertise! You demonstrated comprehensive understanding across multiple skill areas with detailed, accurate responses."
            elif final_score >= 3.0:
                performance_feedback = "Strong Excel knowledge with good understanding of key concepts. Some areas could benefit from additional depth or practice."
            elif final_score >= 2.0:
                performance_feedback = "Basic Excel competency demonstrated. Focus on strengthening fundamental concepts and exploring advanced features."
            else:
                performance_feedback = "Excel skills need significant development. Recommend comprehensive Excel training starting with basic functions and formulas."
            
            report += f"""
### Overall Assessment
{performance_feedback}

### Strengths Identified
"""
            
            # Collect all strengths
            all_strengths = []
            for eval_data in evaluations:
                all_strengths.extend(eval_data.get("strengths", []))
            
            # Get unique strengths
            unique_strengths = list(set(all_strengths))[:5]
            for strength in unique_strengths:
                report += f"- {strength}\n"
            
            report += f"""
### Areas for Improvement
"""
            
            # Collect all improvements
            all_improvements = []
            for eval_data in evaluations:
                all_improvements.extend(eval_data.get("areas_for_improvement", []))
            
            # Get unique improvements
            unique_improvements = list(set(all_improvements))[:5]
            for improvement in unique_improvements:
                report += f"- {improvement}\n"
            
            # Question-by-question analysis
            report += f"""
## Detailed Question Analysis
"""
            
            for i, evaluation in enumerate(evaluations, 1):
                score = evaluation.get("score", 0)
                reasoning = evaluation.get("reasoning", "No reasoning provided")
                keywords_found = evaluation.get("keywords_found", [])
                
                report += f"""
### Question {i}
- **Score:** {score}/5.0
- **Analysis:** {reasoning}
"""
                
                if keywords_found:
                    report += f"- **Key Terms Used:** {', '.join(keywords_found[:5])}\n"
                
                strengths = evaluation.get("strengths", [])
                if strengths:
                    report += f"- **Strengths:** {', '.join(strengths[:3])}\n"
                
                improvements = evaluation.get("areas_for_improvement", [])
                if improvements:
                    report += f"- **Areas for Improvement:** {', '.join(improvements[:2])}\n"
            
            # Recommendations
            report += f"""
## Learning Recommendations

Based on your performance level ({performance_level}), consider:

### Immediate Next Steps
1. **Practice Excel formulas** - Focus on {', '.join(list(skills_covered.keys())[:3]).replace('_', ' ')}
2. **Work with real datasets** - Apply concepts to practical business scenarios
3. **Explore advanced features** - Pivot tables, conditional formatting, data validation

### Long-term Development
1. **Take structured Excel courses** - Focus on areas scoring below 3.0
2. **Practice with Excel certification materials** - Microsoft Office Specialist certification
3. **Join Excel communities** - Online forums and practice groups
4. **Work on projects** - Apply Excel skills to real business problems

### Resources
- Microsoft Excel Help Center
- Excel online training courses
- Practice datasets and exercises
- Excel keyboard shortcuts and productivity tips

---
**Report Generated:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC
**Session Duration:** {duration} minutes
**Total Questions:** {questions_completed}
**Assessment Confidence:** {'High' if self.evaluation_engine else 'Medium (Fallback Mode)'}
"""
            
            return report
            
        except Exception as e:
            self.logger.error(f"Failed to generate report: {e}")
            return f"Report generation failed: {str(e)}"

# =============================================================================
# VOICE-ENHANCED ORCHESTRATOR (Complete with all existing functionality)
# =============================================================================

class VoiceEnhancedInterviewOrchestrator(InterviewOrchestrator):
    """Voice-enhanced interview orchestrator with ALL existing functionality preserved"""
    
    def __init__(self, evaluation_engine=None, question_bank=None, voice_service=None):
        super().__init__(evaluation_engine, question_bank)
        self.voice_service = voice_service
        self.voice_sessions = {}  # Track voice-specific data
        
        # Voice-specific stats
        self.voice_stats = {
            "voice_interviews_started": 0,
            "audio_responses_processed": 0,
            "tts_requests": 0,
            "tts_successes": 0
        }
        
        # Create voice directories
        for directory in ['voice_cache', 'audio_responses', 'temp_audio']:
            Path(directory).mkdir(exist_ok=True)
        
        self.logger.info("âœ… VoiceEnhancedInterviewOrchestrator initialized")
    
    async def start_voice_interview(
        self, 
        candidate_name: str = None,
        voice_id: str = "en-US-sarah",
        audio_mode: bool = True
    ) -> Dict[str, Any]:
        """Start voice-enabled interview with comprehensive audio generation"""
        
        try:
            # Start regular interview first (preserves all existing functionality)
            result = await self.start_interview(candidate_name)
            
            if not result.get("success"):
                return result
            
            session_id = result["session_id"]
            
            # Update voice stats
            self.voice_stats["voice_interviews_started"] += 1
            
            # Add voice capabilities
            if audio_mode and self.voice_service and self.voice_service.available:
                # Initialize voice session
                self.voice_sessions[session_id] = {
                    "audio_files": [],
                    "transcriptions": [],
                    "voice_responses": {},
                    "preferred_voice_id": voice_id,
                    "audio_generation_log": [],
                    "voice_session_start": datetime.utcnow()
                }
                
                # Generate welcome audio
                welcome_text = f"Hello {candidate_name or 'candidate'}! Welcome to your Excel skills assessment. I will be asking you questions about Excel, and you can respond either by voice or text. Let's begin with the first question."
                
                self.voice_stats["tts_requests"] += 1
                welcome_audio = await self.voice_service.text_to_speech(
                    text=welcome_text,
                    voice_id=voice_id
                )
                
                if welcome_audio and welcome_audio.get("success"):
                    self.voice_stats["tts_successes"] += 1
                    self.voice_sessions[session_id]["audio_generation_log"].append({
                        "type": "welcome",
                        "text": welcome_text,
                        "success": True,
                        "timestamp": datetime.utcnow().isoformat()
                    })
                
                # Generate first question audio
                first_question = result.get("first_question")
                question_audio = None
                if first_question:
                    self.voice_stats["tts_requests"] += 1
                    question_audio = await self.voice_service.text_to_speech(
                        text=first_question["text"],
                        voice_id=voice_id
                    )
                    
                    if question_audio and question_audio.get("success"):
                        self.voice_stats["tts_successes"] += 1
                        self.voice_sessions[session_id]["audio_generation_log"].append({
                            "type": "question",
                            "question_id": first_question["id"],
                            "text": first_question["text"],
                            "success": True,
                            "timestamp": datetime.utcnow().isoformat()
                        })
                
                # Enhance result with voice data
                result.update({
                    "audio_mode": True,
                    "voice_features_available": True,
                    "welcome_audio": welcome_audio,
                    "question_audio": question_audio,
                    "voice_id": voice_id,
                    "voice_session_id": session_id
                })
            else:
                result.update({
                    "audio_mode": False,
                    "voice_features_available": False,
                    "note": "Voice service not available - using text mode"
                })
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to start voice interview: {e}")
            return {
                "success": False,
                "error": str(e),
                "status": "error"
            }
    
    async def submit_voice_response(
        self,
        session_id: str,
        response_text: str = None,
        audio_file_path: str = None,
        time_taken: int = 0
    ) -> Dict[str, Any]:
        """Submit voice response with comprehensive audio feedback generation"""
        
        try:
            # Handle transcription if audio provided but no text
            if audio_file_path and not response_text:
                # For now, require text response since Murf doesn't do STT
                response_text = "Audio response provided - transcription not available with current setup"
            
            if not response_text:
                response_text = "Voice response submitted"
            
            # Use existing submit_response method (preserves all functionality)
            result = await self.submit_response(session_id, response_text, time_taken)
            
            if not result.get("success"):
                return result
            
            # Update voice stats
            self.voice_stats["audio_responses_processed"] += 1
            
            # Add voice features to response
            if session_id in self.voice_sessions and self.voice_service and self.voice_service.available:
                voice_session = self.voice_sessions[session_id]
                voice_id = voice_session.get("preferred_voice_id", "en-US-sarah")
                
                # Store audio file reference if provided
                if audio_file_path:
                    voice_session["audio_files"].append({
                        "question_id": self.sessions[session_id]["current_question"]["id"],
                        "audio_path": audio_file_path,
                        "response_text": response_text,
                        "timestamp": datetime.utcnow().isoformat()
                    })
                
                # Generate voice feedback
                evaluation = result.get("evaluation", {})
                if evaluation:
                    score = evaluation.get("score", 0)
                    reasoning = evaluation.get("reasoning", "")
                    
                    # Create contextual feedback
                    if score >= 4.0:
                        feedback_prefix = "Excellent response!"
                    elif score >= 3.0:
                        feedback_prefix = "Good answer!"
                    elif score >= 2.0:
                        feedback_prefix = "Not bad, but could be improved."
                    else:
                        feedback_prefix = "Let's work on this area."
                    
                    feedback_text = f"{feedback_prefix} Your score is {score} out of 5. {reasoning[:150]}{'...' if len(reasoning) > 150 else ''}"
                    
                    self.voice_stats["tts_requests"] += 1
                    feedback_audio = await self.voice_service.text_to_speech(
                        text=feedback_text,
                        voice_id=voice_id
                    )
                    
                    if feedback_audio and feedback_audio.get("success"):
                        self.voice_stats["tts_successes"] += 1
                        result["feedback_audio"] = feedback_audio
                        
                        voice_session["audio_generation_log"].append({
                            "type": "feedback",
                            "score": score,
                            "text": feedback_text,
                            "success": True,
                            "timestamp": datetime.utcnow().isoformat()
                        })
                
                # Handle completion or next question
                if result.get("status") == "completed":
                    # Generate conclusion audio
                    final_results = result.get("final_results", {})
                    final_score = final_results.get("final_score", 0)
                    questions_completed = final_results.get("questions_completed", 0)
                    performance_level = final_results.get("performance_level", "Unknown")
                    
                    conclusion_text = f"Congratulations! You have completed the Excel skills assessment. You answered {questions_completed} questions with a final score of {final_score:.1f} out of 5, achieving {performance_level} level performance. Thank you for participating!"
                    
                    self.voice_stats["tts_requests"] += 1
                    conclusion_audio = await self.voice_service.text_to_speech(
                        text=conclusion_text,
                        voice_id=voice_id
                    )
                    
                    if conclusion_audio and conclusion_audio.get("success"):
                        self.voice_stats["tts_successes"] += 1
                        result["conclusion_audio"] = conclusion_audio
                        
                        voice_session["audio_generation_log"].append({
                            "type": "conclusion",
                            "final_score": final_score,
                            "text": conclusion_text,
                            "success": True,
                            "timestamp": datetime.utcnow().isoformat()
                        })
                
                else:
                    # Generate next question audio
                    next_question = result.get("next_question")
                    if next_question:
                        self.voice_stats["tts_requests"] += 1
                        question_audio = await self.voice_service.text_to_speech(
                            text=next_question["text"],
                            voice_id=voice_id
                        )
                        
                        if question_audio and question_audio.get("success"):
                            self.voice_stats["tts_successes"] += 1
                            result["next_question_audio"] = question_audio
                            
                            voice_session["audio_generation_log"].append({
                                "type": "question",
                                "question_id": next_question["id"],
                                "text": next_question["text"],
                                "success": True,
                                "timestamp": datetime.utcnow().isoformat()
                            })
                
                # Add voice session stats to response
                result["voice_stats"] = {
                    "audio_files_count": len(voice_session["audio_files"]),
                    "audio_generation_count": len(voice_session["audio_generation_log"]),
                    "voice_session_duration": self._calculate_voice_session_duration(voice_session)
                }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to process voice response: {e}")
            return {
                "success": False,
                "error": str(e),
                "status": "error"
            }
    
    def _calculate_voice_session_duration(self, voice_session: Dict) -> int:
        """Calculate voice session duration in minutes"""
        try:
            start_time = voice_session.get("voice_session_start")
            if start_time:
                duration = datetime.utcnow() - start_time
                return int(duration.total_seconds() / 60)
        except:
            pass
        return 0
    
    async def get_voice_session_details(self, session_id: str) -> Dict[str, Any]:
        """Get detailed voice session information"""
        
        try:
            if session_id not in self.voice_sessions:
                return {
                    "success": False,
                    "error": "Voice session not found"
                }
            
            voice_session = self.voice_sessions[session_id]
            
            return {
                "success": True,
                "session_id": session_id,
                "voice_features": {
                    "preferred_voice_id": voice_session.get("preferred_voice_id"),
                    "audio_files_count": len(voice_session.get("audio_files", [])),
                    "audio_generation_count": len(voice_session.get("audio_generation_log", [])),
                    "session_duration_minutes": self._calculate_voice_session_duration(voice_session)
                },
                "audio_generation_log": voice_session.get("audio_generation_log", []),
                "voice_system_stats": self.get_voice_stats()
            }
            
        except Exception as e:
            self.logger.error(f"Failed to get voice session details: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_voice_stats(self) -> Dict[str, Any]:
        """Get comprehensive voice system statistics"""
        
        tts_success_rate = 0
        if self.voice_stats["tts_requests"] > 0:
            tts_success_rate = (self.voice_stats["tts_successes"] / self.voice_stats["tts_requests"]) * 100
        
        return {
            "voice_interviews_started": self.voice_stats["voice_interviews_started"],
            "audio_responses_processed": self.voice_stats["audio_responses_processed"],
            "tts_requests": self.voice_stats["tts_requests"],
            "tts_successes": self.voice_stats["tts_successes"],
            "tts_success_rate": round(tts_success_rate, 2),
            "voice_service_available": self.voice_service and self.voice_service.available,
            "active_voice_sessions": len(self.voice_sessions),
            "voice_cache_files": len(list(Path("voice_cache").glob("*.mp3"))) if Path("voice_cache").exists() else 0
        }
    
    # Inherit ALL existing methods from parent class
    # No existing functionality is removed or modified

# =============================================================================
# FACTORY CLASSES (Enhanced with voice support)
# =============================================================================

class InterviewWorkflowFactory:
    """Enhanced factory for creating interview orchestrators"""
    
    @staticmethod
    def create_complete_workflow(evaluation_engine=None, question_bank=None) -> InterviewOrchestrator:
        """Create complete workflow with all features"""
        return InterviewOrchestrator(evaluation_engine, question_bank)
    
    @staticmethod
    async def create_text_interview(evaluation_engine=None) -> InterviewOrchestrator:
        """Create text-only interview with enhanced features"""
        question_bank = SimpleQuestionBank()
        return InterviewOrchestrator(evaluation_engine, question_bank)
    
    @staticmethod
    async def create_voice_interview(
        evaluation_engine=None, 
        voice_service=None,
        question_bank=None
    ) -> VoiceEnhancedInterviewOrchestrator:
        """Create voice-enabled interview with all functionality"""
        if not question_bank:
            question_bank = SimpleQuestionBank()
        
        return VoiceEnhancedInterviewOrchestrator(
            evaluation_engine=evaluation_engine,
            question_bank=question_bank, 
            voice_service=voice_service
        )
    
    @staticmethod
    async def create_production_system(
        anthropic_api_key: str,
        murf_api_key: str = None,
        voice_enabled: bool = True
    ) -> Union[InterviewOrchestrator, VoiceEnhancedInterviewOrchestrator]:
        """Create production-ready system with all features"""
        
        # Create evaluation engine
        if EVALUATION_ENGINE_AVAILABLE:
            evaluation_engine = ClaudeEvaluationEngine(anthropic_api_key=anthropic_api_key)
        else:
            evaluation_engine = None
        
        # Create question bank
        question_bank = SimpleQuestionBank()
        
        # Create voice service if requested
        voice_service = None
        if voice_enabled and murf_api_key:
            try:
                from main import MurfAPIClient, VoiceService
                murf_client = MurfAPIClient(murf_api_key)
                voice_service = VoiceService(murf_client)
            except ImportError:
                logger.warning("Voice service dependencies not available")
        
        # Return appropriate orchestrator
        if voice_service:
            return VoiceEnhancedInterviewOrchestrator(
                evaluation_engine=evaluation_engine,
                question_bank=question_bank,
                voice_service=voice_service
            )
        else:
            return InterviewOrchestrator(
                evaluation_engine=evaluation_engine,
                question_bank=question_bank
            )

class VoiceInterviewFactory:
    """Factory specifically for voice interviews with all features"""
    
    @staticmethod
    async def create_voice_interview(
        evaluation_engine=None,
        voice_service=None,
        question_bank=None
    ) -> VoiceEnhancedInterviewOrchestrator:
        """Create comprehensive voice interview system"""
        return await InterviewWorkflowFactory.create_voice_interview(
            evaluation_engine=evaluation_engine,
            voice_service=voice_service,
            question_bank=question_bank
        )
    
    @staticmethod
    async def create_complete_voice_system(
        anthropic_api_key: str,
        murf_api_key: str,
        question_bank_path: str = None
    ) -> VoiceEnhancedInterviewOrchestrator:
        """Create complete voice system with all integrations"""
        
        logger.info("Creating complete voice interview system...")
        
        # This integrates with actual services
        return await InterviewWorkflowFactory.create_production_system(
            anthropic_api_key=anthropic_api_key,
            murf_api_key=murf_api_key,
            voice_enabled=True
        )

# =============================================================================
# COMPREHENSIVE TESTING FUNCTIONS
# =============================================================================

async def test_interview_orchestrator():
    """Comprehensive test of the interview orchestrator"""
    
    print("ðŸ§ª Testing Complete Interview Orchestrator")
    print("=" * 60)
    
    try:
        # Test basic orchestrator
        print("ðŸ“ Testing basic orchestrator...")
        orchestrator = InterviewOrchestrator()
        
        # Test starting interview
        print("ðŸ“ Testing interview start...")
        start_result = await orchestrator.start_interview("Test Candidate", "intermediate")
        
        if start_result.get("success"):
            session_id = start_result["session_id"]
            print(f"âœ… Interview started: {session_id}")
            print(f"   First question: {start_result['first_question']['id']}")
            
            # Test submitting multiple responses
            test_responses = [
                "VLOOKUP is a vertical lookup function that searches for a value in the leftmost column of a table and returns a value in the same row from a specified column. INDEX-MATCH is more flexible because it can search in any direction.",
                "To create a pivot table, you select your data, go to Insert > Pivot Table, then drag fields to the Rows, Columns, Values, and Filters areas to summarize your data.",
                "Common Excel errors include #N/A when lookup values aren't found, #REF! when cell references are invalid, and #VALUE! when formula arguments are wrong. You can use IFERROR to handle these gracefully."
            ]
            
            for i, response in enumerate(test_responses):
                print(f"ðŸ“ Testing response {i+1}...")
                response_result = await orchestrator.submit_response(
                    session_id=session_id,
                    response_text=response,
                    time_taken=60 + (i * 30)
                )
                
                if response_result.get("success"):
                    evaluation = response_result["evaluation"]
                    print(f"âœ… Response {i+1} processed - Score: {evaluation['score']}/5.0")
                    
                    if response_result.get("status") == "completed":
                        print("ðŸŽ‰ Interview completed!")
                        break
                else:
                    print(f"âŒ Response {i+1} failed: {response_result.get('error')}")
            
            # Test session status
            print("ðŸ“ Testing session status...")
            status_result = await orchestrator.get_session_status(session_id)
            if status_result.get("success"):
                print(f"âœ… Session status: {status_result['status']}")
                print(f"   Questions completed: {status_result['questions_completed']}")
                print(f"   Current score: {status_result['current_score']:.1f}/5.0")
            
            # Test report generation
            print("ðŸ“ Testing report generation...")
            report = await orchestrator.generate_final_report(session_id)
            if "Report generation failed" not in report:
                print("âœ… Report generated successfully")
                print(f"   Report length: {len(report)} characters")
            else:
                print(f"âŒ Report generation failed")
            
            # Test system stats
            print("ðŸ“ Testing system stats...")
            stats = orchestrator.get_system_stats()
            print(f"âœ… System stats: {stats['interviews_started']} started, {stats['interviews_completed']} completed")
            
        else:
            print(f"âŒ Interview start failed: {start_result.get('error')}")
        
        print("\nâœ… Basic orchestrator test completed!")
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_voice_orchestrator():
    """Test voice-enhanced orchestrator"""
    
    print("ðŸŽ™ï¸ Testing Voice-Enhanced Orchestrator")
    print("=" * 60)
    
    try:
        # Create mock voice service for testing
        class MockVoiceService:
            def __init__(self):
                self.available = True
            
            async def text_to_speech(self, text, voice_id=None):
                return {
                    "success": True,
                    "audio_path": f"/tmp/mock_audio_{uuid.uuid4().hex[:8]}.mp3",
                    "audio_url": f"/audio/mock_audio_{uuid.uuid4().hex[:8]}.mp3",
                    "voice_id": voice_id or "en-US-sarah"
                }
        
        voice_service = MockVoiceService()
        voice_orchestrator = VoiceEnhancedInterviewOrchestrator(
            evaluation_engine=None,
            question_bank=SimpleQuestionBank(),
            voice_service=voice_service
        )
        
        print("ðŸ“ Testing voice interview start...")
        start_result = await voice_orchestrator.start_voice_interview(
            candidate_name="Voice Test User",
            voice_id="en-US-sarah"
        )
        
        if start_result.get("success"):
            session_id = start_result["session_id"]
            print(f"âœ… Voice interview started: {session_id}")
            print(f"   Audio mode: {start_result.get('audio_mode', False)}")
            print(f"   Welcome audio: {start_result.get('welcome_audio', {}).get('success', False)}")
            
            # Test voice response
            print("ðŸ“ Testing voice response submission...")
            voice_response_result = await voice_orchestrator.submit_voice_response(
                session_id=session_id,
                response_text="VLOOKUP searches vertically in tables while INDEX-MATCH is more flexible for lookups",
                time_taken=90
            )
            
            if voice_response_result.get("success"):
                print("âœ… Voice response processed successfully")
                print(f"   Feedback audio: {voice_response_result.get('feedback_audio', {}).get('success', False)}")
                print(f"   Next question audio: {voice_response_result.get('next_question_audio', {}).get('success', False)}")
            else:
                print(f"âŒ Voice response failed: {voice_response_result.get('error')}")
            
            # Test voice stats
            voice_stats = voice_orchestrator.get_voice_stats()
            print(f"âœ… Voice stats: {voice_stats['voice_interviews_started']} started, {voice_stats['tts_success_rate']:.1f}% TTS success")
        
        else:
            print(f"âŒ Voice interview start failed: {start_result.get('error')}")
        
        print("\nâœ… Voice orchestrator test completed!")
        return True
        
    except Exception as e:
        print(f"âŒ Voice test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ðŸŽ­ Complete Interview Orchestrator with Voice Integration")
    print("=" * 70)
    print("âœ… Features:")
    print("  â€¢ Complete session management with enhanced tracking")
    print("  â€¢ Built-in question bank (10 Excel questions)")
    print("  â€¢ Fallback evaluation system with contextual feedback")
    print("  â€¢ Full voice integration support with audio generation") 
    print("  â€¢ Comprehensive error handling and recovery")
    print("  â€¢ Session status tracking and analytics")
    print("  â€¢ Detailed report generation with performance analysis")
    print("  â€¢ Voice session management and audio file tracking")
    print("  â€¢ Production-ready with all existing functionality preserved")
    
    # Run comprehensive tests
    async def run_all_tests():
        basic_success = await test_interview_orchestrator()
        voice_success = await test_voice_orchestrator()
        return basic_success and voice_success
    
    success = asyncio.run(run_all_tests())
    print(f"\n{'ðŸŽ‰ All tests passed!' if success else 'âŒ Some tests failed'}")
    print("\nðŸš€ Ready for production use with both text and voice interviews!")